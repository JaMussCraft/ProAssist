import re, json

EVALUATION_SYS_PROMPT = """You are an expert in evaluating the quality of user-assistant dialogues. Your task is to evaluate dialog responses generated by an assistant model that helps users with their tasks. You should evaluate the dialogs by comparing them to reference gold-standard dialogues from professional assistants.

Requirement:
1. Read dialogues carefully and compare them line by line. Keep you analysis concise and to the point.
2. Evaluate the following aspects: 
- Correctness: does each generated instruction/feedback make sense (correct or relevant) or not, based on the context and the gold-standard reference?
- Promptness: does the assistant provide guidance at the right time, or does it talk too early or too late?
- Efficiency: does the assistant provide the necessary information in a concise and efficient manner, without too much repetition or redundancy information?
- Overall: the overall helpfulness and quality of the assistant's responses.
3. For each aspect, give a score from 1 to 5 based on the following criteria:
- 1=very poor: most of utterances are incorrect, irrelevant, mistimed, inefficient etc
- 2=poor: bad utterances that are incorrect, irrelevant, mistimed are more than good ones
- 3=average: the number of good and bad utterances are roughly the same
- 4=good: more good utterances than bad ones
- 5=excellent: most of utterances are correct, relevant, timely, efficient etc
"""

DIALOG_EVALUATION_PROMPT_TEMPLATE = """Gold-standard reference dialogue:
{reference_dialog}

Generated dialogue by the model:
{generated_dialog}

Format your answer as follows:
<your step-by-step comparison and concise analysis>
---
{{"correctness": x, "promptness": y, "efficiency": z, "overall": w}}
"""


LLM_EVAL_METRICS = ["correctness", "promptness", "efficiency", "overall"]


def parse_scores(text: str) -> dict | None:
    scores = {}
    if "---" in text:
        text = text.split("---")[1]
    try:
        text = re.findall(r"\{.*?\}", text)[0]
        scores = json.loads(text)
    except:
        return None
    return scores
